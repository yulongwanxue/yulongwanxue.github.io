# Chain of Thought (CoT) æŽ¨ç†æ¡†æž¶ï¼šå¤§è¯­è¨€æ¨¡åž‹çš„æ€ç»´é“¾æŠ€æœ¯æ·±åº¦è§£æž

## æ‘˜è¦

Chain of Thought (CoT) æ˜¯ä¸€ç§é©å‘½æ€§çš„æç¤ºå·¥ç¨‹æŠ€æœ¯ï¼Œå®ƒé€šè¿‡å¼•å¯¼å¤§è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰ä»¥é€æ­¥æŽ¨ç†çš„æ–¹å¼è§£å†³å¤æ‚é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†æ¨¡åž‹åœ¨æ•°å­¦æŽ¨ç†ã€é€»è¾‘åˆ†æžå’Œå¤šæ­¥éª¤é—®é¢˜æ±‚è§£ç­‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚æœ¬æ–‡å°†æ·±å…¥æŽ¢è®¨ CoT æŠ€æœ¯çš„æ ¸å¿ƒåŽŸç†ã€å·¥ç¨‹å®žçŽ°ä»¥åŠåœ¨ç”Ÿäº§çŽ¯å¢ƒä¸­çš„åº”ç”¨æ¨¡å¼ã€‚

## ç›®å½•

1. [CoT æŠ€æœ¯æ¦‚è¿°](#1-cot-æŠ€æœ¯æ¦‚è¿°)
2. [æ ¸å¿ƒåŽŸç†ä¸Žç†è®ºåŸºç¡€](#2-æ ¸å¿ƒåŽŸç†ä¸Žç†è®ºåŸºç¡€)
3. [å·¥ç¨‹å®žçŽ°æž¶æž„](#3-å·¥ç¨‹å®žçŽ°æž¶æž„)
4. [çŠ¶æ€ç®¡ç†ä¸Žæ•°æ®æµ](#4-çŠ¶æ€ç®¡ç†ä¸Žæ•°æ®æµ)
5. [æŽ¨ç†æ‰§è¡Œå¼•æ“Ž](#5-æŽ¨ç†æ‰§è¡Œå¼•æ“Ž)
6. [éªŒè¯ä¸Žè´¨é‡ä¿è¯](#6-éªŒè¯ä¸Žè´¨é‡ä¿è¯)
7. [LangGraph å·¥ä½œæµç¼–æŽ’](#7-langgraph-å·¥ä½œæµç¼–æŽ’)
8. [æ€§èƒ½ä¼˜åŒ–ç­–ç•¥](#8-æ€§èƒ½ä¼˜åŒ–ç­–ç•¥)
9. [å®žé™…åº”ç”¨åœºæ™¯](#9-å®žé™…åº”ç”¨åœºæ™¯)
10. [æœ€ä½³å®žè·µä¸Žå±€é™æ€§](#10-æœ€ä½³å®žè·µä¸Žå±€é™æ€§)

---

## 1. CoT æŠ€æœ¯æ¦‚è¿°

### 1.1 ä»€ä¹ˆæ˜¯ Chain of Thoughtï¼Ÿ

Chain of Thoughtï¼ˆæ€ç»´é“¾ï¼‰æ˜¯ä¸€ç§æç¤ºå·¥ç¨‹æŠ€æœ¯ï¼Œæœ€åˆç”± Google Research åœ¨ 2022 å¹´æå‡ºã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡åœ¨æç¤ºä¸­åŠ å…¥ä¸­é—´æŽ¨ç†æ­¥éª¤ï¼Œå¼•å¯¼å¤§è¯­è¨€æ¨¡åž‹æ¨¡æ‹Ÿäººç±»çš„æ€ç»´è¿‡ç¨‹ï¼Œå°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºå¤šä¸ªå¯ç®¡ç†çš„å­æ­¥éª¤ã€‚

### 1.2 ä¸ºä»€ä¹ˆéœ€è¦ CoTï¼Ÿ

ä¼ ç»Ÿçš„ç«¯åˆ°ç«¯ï¼ˆend-to-endï¼‰æç¤ºæ–¹æ³•åœ¨å¤„ç†å¤æ‚æŽ¨ç†ä»»åŠ¡æ—¶å­˜åœ¨æ˜¾è‘—å±€é™æ€§ï¼š

- **é»‘ç›’æŽ¨ç†**ï¼šæ¨¡åž‹ç›´æŽ¥ç»™å‡ºç­”æ¡ˆï¼Œç¼ºä¹å¯è§£é‡Šæ€§
- **é”™è¯¯ç´¯ç§¯**ï¼šå¤æ‚é—®é¢˜å®¹æ˜“åœ¨æŸä¸ªéšå¼æ­¥éª¤å‡ºé”™
- **æŽ¨ç†èƒ½åŠ›å—é™**ï¼šå¤šæ­¥éª¤é€»è¾‘æŽ¨ç†å‡†ç¡®çŽ‡ä½Ž

CoT é€šè¿‡æ˜¾å¼åŒ–æŽ¨ç†è¿‡ç¨‹ï¼Œè§£å†³äº†è¿™äº›é—®é¢˜ï¼Œä½¿æ¨¡åž‹çš„æŽ¨ç†è¿‡ç¨‹é€æ˜ŽåŒ–ã€å¯éªŒè¯ã€‚

### 1.3 CoT çš„æ¼”è¿›

- **Zero-shot CoT**ï¼šé€šè¿‡æ·»åŠ  "Let's think step by step" ç­‰è§¦å‘è¯­å¥
- **Few-shot CoT**ï¼šåœ¨æç¤ºä¸­æä¾›å¸¦æŽ¨ç†è¿‡ç¨‹çš„ç¤ºä¾‹
- **Auto-CoT**ï¼šè‡ªåŠ¨ç”ŸæˆæŽ¨ç†é“¾ç¤ºä¾‹
- **Self-Consistency**ï¼šé€šè¿‡å¤šæ¬¡é‡‡æ ·å’ŒæŠ•ç¥¨æå‡å‡†ç¡®æ€§
- **Tree of Thoughts (ToT)**ï¼šæŽ¢ç´¢å¤šä¸ªæŽ¨ç†è·¯å¾„çš„æ ‘çŠ¶ç»“æž„

---

## 2. æ ¸å¿ƒåŽŸç†ä¸Žç†è®ºåŸºç¡€

### 2.1 æ€ç»´é“¾çš„è®¤çŸ¥æ¨¡åž‹

CoT åŸºäºŽä»¥ä¸‹è®¤çŸ¥ç§‘å­¦åŽŸç†ï¼š

**å·¥ä½œè®°å¿†ç†è®º**ï¼šäººç±»è§£å†³å¤æ‚é—®é¢˜æ—¶ï¼Œä¼šå°†é—®é¢˜åˆ†è§£ä¸ºç¬¦åˆå·¥ä½œè®°å¿†å®¹é‡çš„å°å—ã€‚CoT é€šè¿‡æ­¥éª¤åˆ†è§£ï¼Œæ¨¡æ‹Ÿäº†è¿™ä¸€è¿‡ç¨‹ã€‚

**å…ƒè®¤çŸ¥ç›‘æŽ§**ï¼šCoT å…è®¸æ¨¡åž‹åœ¨æŽ¨ç†è¿‡ç¨‹ä¸­"è‡ªæˆ‘å®¡è§†"æ¯ä¸€æ­¥çš„æ­£ç¡®æ€§ï¼Œç±»ä¼¼äººç±»çš„å…ƒè®¤çŸ¥è¿‡ç¨‹ã€‚

### 2.2 æç¤ºå·¥ç¨‹åŽŸç†

```python
self.cot_prompt = ChatPromptTemplate.from_messages([
    ("system", """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŽ¨ç†åŠ©æ‰‹ï¼Œéœ€è¦ä½¿ç”¨æ€ç»´é“¾æ–¹æ³•é€æ­¥è§£å†³é—®é¢˜ã€‚
    è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡ŒæŽ¨ç†ï¼š
    1. åˆ†æžé—®é¢˜çš„å…³é”®è¦ç´ 
    2. åˆ—å‡ºéœ€è¦è€ƒè™‘çš„æŽ¨ç†æ­¥éª¤
    3. æ‰§è¡Œæ¯ä¸€æ­¥æŽ¨ç†
    4. éªŒè¯æŽ¨ç†ç»“æžœ
    5. ç»™å‡ºæœ€ç»ˆç­”æ¡ˆ"""),
    ("human", """é—®é¢˜: {question}

    è¯·æŒ‰ç…§æ€ç»´é“¾æ–¹æ³•é€æ­¥åˆ†æžè¿™ä¸ªé—®é¢˜ï¼Œå¹¶ç»™å‡ºè¯¦ç»†çš„æŽ¨ç†è¿‡ç¨‹ã€‚""")
])
```

è¿™ä¸ªæç¤ºæ¨¡æ¿éµå¾ªäº†å‡ ä¸ªå…³é”®è®¾è®¡åŽŸåˆ™ï¼š

1. **æ˜Žç¡®è§’è‰²å®šä½**ï¼šå®šä¹‰æ¨¡åž‹ä¸º"æŽ¨ç†åŠ©æ‰‹"
2. **ç»“æž„åŒ–æŒ‡ä»¤**ï¼šæä¾›æ¸…æ™°çš„æŽ¨ç†æ¡†æž¶
3. **æ ¼å¼è§„èŒƒ**ï¼šè¦æ±‚ç‰¹å®šçš„è¾“å‡ºæ ¼å¼ä¾¿äºŽè§£æž
4. **ä»»åŠ¡åˆ†è§£**ï¼šå°†æŽ¨ç†è¿‡ç¨‹åˆ†ä¸º 5 ä¸ªæ˜Žç¡®æ­¥éª¤

---

## 3. å·¥ç¨‹å®žçŽ°æž¶æž„

### 3.1 ç³»ç»Ÿæž¶æž„æ¦‚è§ˆ

CoT æŽ¨ç†æ¡†æž¶é‡‡ç”¨äº†åŸºäºŽçŠ¶æ€æœºçš„å·¥ä½œæµæž¶æž„ï¼Œä¸»è¦åŒ…å«ä»¥ä¸‹ç»„ä»¶ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CoT Agent (æŽ§åˆ¶å±‚)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  - é—®é¢˜åˆ†æžå™¨ (analyze_question)                â”‚
â”‚  - æŽ¨ç†æ‰§è¡Œå™¨ (execute_reasoning_step)          â”‚
â”‚  - éªŒè¯å¼•æ“Ž (validate_and_conclude)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           LangGraph å·¥ä½œæµå¼•æ“Ž                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  - çŠ¶æ€ç®¡ç† (StateGraph)                         â”‚
â”‚  - èŠ‚ç‚¹ç¼–æŽ’ (Nodes)                             â”‚
â”‚  - æ¡ä»¶è·¯ç”± (Conditional Edges)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              LLM æŽ¥å£å±‚                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  - ChatOpenAI (langchain_openai)                â”‚
â”‚  - æç¤ºæ¨¡æ¿ç®¡ç† (ChatPromptTemplate)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 æ ¸å¿ƒç±»è®¾è®¡

```python
class CoTAgent:
    def __init__(self, model_name: str = "gpt-3.5-turbo"):
        self.llm = ChatOpenAI(
            model_name="gpt-4.1",
            base_url=baseurl,
            api_key=apikey,
            temperature=0.1
        )
        self.cot_prompt = ChatPromptTemplate.from_messages([...])
        self.validation_prompt = ChatPromptTemplate.from_messages([...])
```

**è®¾è®¡è¦ç‚¹**ï¼š

- **ä½Žæ¸©åº¦å‚æ•°**ï¼š`temperature=0.1` ç¡®ä¿æŽ¨ç†è¿‡ç¨‹çš„ç¡®å®šæ€§å’Œç¨³å®šæ€§
- **æ¨¡æ¿åˆ†ç¦»**ï¼šå°†æŽ¨ç†æç¤ºå’ŒéªŒè¯æç¤ºåˆ†ç¦»ï¼Œå®žçŽ°å…³æ³¨ç‚¹åˆ†ç¦»
- **ä¾èµ–æ³¨å…¥**ï¼šé€šè¿‡æž„é€ å‡½æ•°æ³¨å…¥æ¨¡åž‹é…ç½®ï¼Œä¾¿äºŽæµ‹è¯•å’Œæ›¿æ¢

---

## 4. çŠ¶æ€ç®¡ç†ä¸Žæ•°æ®æµ

### 4.1 çŠ¶æ€å®šä¹‰

```python
class CoTState(TypedDict):
    question: str                          # åŽŸå§‹é—®é¢˜
    thoughts: List[str]                    # æ€è€ƒè¿‡ç¨‹è®°å½•
    reasoning_steps: List[Dict[str, Any]]  # ç»“æž„åŒ–æŽ¨ç†æ­¥éª¤
    final_answer: str                      # æœ€ç»ˆç­”æ¡ˆ
    current_step: int                      # å½“å‰æ‰§è¡Œæ­¥éª¤ç´¢å¼•
    is_complete: bool                      # å®Œæˆæ ‡å¿—
```

è¿™ç§çŠ¶æ€è®¾è®¡ä½“çŽ°äº†å‡ ä¸ªå…³é”®åŽŸåˆ™ï¼š

**ä¸å¯å˜æ€§**ï¼šæ¯ä¸ªèŠ‚ç‚¹è¿”å›žçŠ¶æ€å‰¯æœ¬è€Œéžç›´æŽ¥ä¿®æ”¹ï¼Œä¿è¯äº†çŠ¶æ€è½¬æ¢çš„å¯è¿½æº¯æ€§ã€‚

**å®Œæ•´æ€§**ï¼šçŠ¶æ€åŒ…å«äº†æŽ¨ç†è¿‡ç¨‹çš„æ‰€æœ‰å¿…è¦ä¿¡æ¯ï¼Œæ”¯æŒæ–­ç‚¹ç»­æŽ¨å’Œå›žæº¯ã€‚

**å¯è§‚æµ‹æ€§**ï¼š`current_step` å’Œ `is_complete` å­—æ®µä½¿å¾—æŽ¨ç†è¿›åº¦é€æ˜ŽåŒ–ã€‚

### 4.2 æŽ¨ç†æ­¥éª¤æ•°æ®ç»“æž„

```python
{
    "step_number": "æ­¥éª¤1",           # æ­¥éª¤æ ‡è¯†
    "content": "åˆ†æžé—®é¢˜å…³é”®è¦ç´ ",     # æ­¥éª¤æè¿°
    "status": "completed",           # æ‰§è¡ŒçŠ¶æ€ï¼špending/completed
    "executed_content": "..."        # å®žé™…æ‰§è¡Œç»“æžœ
}
```

è¿™ç§ç»“æž„æ”¯æŒäº†ï¼š

- **çŠ¶æ€è¿½è¸ª**ï¼šè®°å½•æ¯ä¸ªæ­¥éª¤çš„æ‰§è¡ŒçŠ¶æ€
- **ç»“æžœç¼“å­˜**ï¼š`executed_content` å­˜å‚¨æ‰§è¡Œç»“æžœï¼Œé¿å…é‡å¤è®¡ç®—
- **é”™è¯¯æ¢å¤**ï¼šå¤±è´¥æ­¥éª¤å¯ä»¥é‡æ–°æ‰§è¡Œè€Œä¸å½±å“å·²å®Œæˆæ­¥éª¤

---

## 5. æŽ¨ç†æ‰§è¡Œå¼•æ“Ž

### 5.1 é—®é¢˜åˆ†æžé˜¶æ®µ

```python
def analyze_question(self, state: CoTState) -> CoTState:
    chain = self.cot_prompt | self.llm
    response = chain.invoke({"question": state["question"]})

    steps = self._extract_reasoning_steps(response.content)

    updated_state = state.copy()
    updated_state["reasoning_steps"] = steps
    updated_state["current_step"] = 0
    updated_state["is_complete"] = False

    return updated_state
```

**å…³é”®æŠ€æœ¯ç‚¹**ï¼š

1. **LangChain LCEL è¯­æ³•**ï¼šä½¿ç”¨ `|` æ“ä½œç¬¦æž„å»ºå¤„ç†é“¾
2. **æ­¥éª¤æå–**ï¼šé€šè¿‡æ­£åˆ™æ¨¡å¼è¯†åˆ« "æ­¥éª¤X:" æ ¼å¼
3. **çŠ¶æ€åˆå§‹åŒ–**ï¼šè®¾ç½®åˆå§‹æ‰§è¡Œä½ç½®å’Œå®Œæˆæ ‡å¿—

### 5.2 æ­¥éª¤è§£æžç®—æ³•

```python
def _extract_reasoning_steps(self, text: str) -> List[Dict[str, Any]]:
    steps = []
    lines = text.split('\n')

    for line in lines:
        if line.strip().startswith('æ­¥éª¤') and ':' in line:
            step_info = line.split(':', 1)
            steps.append({
                "step_number": step_info[0].strip(),
                "content": step_info[1].strip(),
                "status": "pending",
                "executed_content": None
            })
        elif line.strip().startswith('æœ€ç»ˆç­”æ¡ˆ'):
            answer = line.split(':', 1)[1].strip()
            steps.append({
                "step_number": "final",
                "content": answer,
                "status": "pending",
                "executed_content": None
            })

    return steps
```

è¿™ä¸ªè§£æžå™¨é‡‡ç”¨äº†ç®€å•ä½†æœ‰æ•ˆçš„åŸºäºŽè§„åˆ™çš„æ–¹æ³•ã€‚åœ¨ç”Ÿäº§çŽ¯å¢ƒä¸­ï¼Œå¯ä»¥è€ƒè™‘ï¼š

- **ç»“æž„åŒ–è¾“å‡º**ï¼šä½¿ç”¨ Pydantic æ¨¡åž‹å’Œ LLM çš„ç»“æž„åŒ–è¾“å‡ºèƒ½åŠ›
- **é”™è¯¯å¤„ç†**ï¼šå¢žåŠ æ ¼å¼éªŒè¯å’Œå¼‚å¸¸å¤„ç†
- **å¤šè¯­è¨€æ”¯æŒ**ï¼šæ”¯æŒè‹±æ–‡ç­‰å…¶ä»–è¯­è¨€çš„æ­¥éª¤æ ‡è®°

### 5.3 æ­¥éª¤æ‰§è¡Œå¼•æ“Ž

```python
def execute_reasoning_step(self, state: CoTState) -> CoTState:
    current_step_index = state["current_step"]

    if current_step_index >= len(state["reasoning_steps"]):
        updated_state = state.copy()
        updated_state["is_complete"] = True
        return updated_state

    current_step = state["reasoning_steps"][current_step_index]
    executed_content = self._execute_single_step(current_step, state["question"])

    updated_steps = state["reasoning_steps"].copy()
    updated_steps[current_step_index] = {
        **current_step,
        "status": "completed",
        "executed_content": executed_content
    }

    updated_state = state.copy()
    updated_state["reasoning_steps"] = updated_steps
    updated_state["current_step"] = current_step_index + 1
    updated_state["thoughts"].append(f"æ­¥éª¤{current_step['step_number']}: {executed_content}")

    return updated_state
```

**æ‰§è¡Œæµç¨‹**ï¼š

1. **è¾¹ç•Œæ£€æŸ¥**ï¼šåˆ¤æ–­æ˜¯å¦æ‰€æœ‰æ­¥éª¤å·²å®Œæˆ
2. **æ­¥éª¤æ‰§è¡Œ**ï¼šè°ƒç”¨ LLM æ‰§è¡Œå•ä¸ªæŽ¨ç†æ­¥éª¤
3. **çŠ¶æ€æ›´æ–°**ï¼šæ›´æ–°æ­¥éª¤çŠ¶æ€å’Œæ‰§è¡Œç»“æžœ
4. **ç´¢å¼•é€’å¢ž**ï¼šç§»åŠ¨åˆ°ä¸‹ä¸€æ­¥éª¤
5. **æ€è€ƒè®°å½•**ï¼šä¿å­˜æŽ¨ç†ç—•è¿¹

### 5.4 å•æ­¥æ‰§è¡Œå®žçŽ°

```python
def _execute_single_step(self, step: Dict, question: str) -> str:
    execution_prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŽ¨ç†æ‰§è¡ŒåŠ©æ‰‹ï¼Œéœ€è¦è¯¦ç»†æ‰§è¡Œç»™å®šçš„æŽ¨ç†æ­¥éª¤ã€‚"),
        ("human", f"""é—®é¢˜: {question}
        æŽ¨ç†æ­¥éª¤: {step['content']}

        è¯·è¯¦ç»†æ‰§è¡Œè¿™ä¸ªæŽ¨ç†æ­¥éª¤ï¼Œå¹¶ç»™å‡ºå…·ä½“çš„ç»“æžœã€‚""")
    ])

    chain = execution_prompt | self.llm
    response = chain.invoke({})
    return response.content
```

è¿™é‡Œé‡‡ç”¨äº†**ä¸Šä¸‹æ–‡ä¿æŒ**ç­–ç•¥ï¼šæ¯æ¬¡æ‰§è¡Œéƒ½ä¼ å…¥åŽŸå§‹é—®é¢˜ï¼Œç¡®ä¿ LLM ä¸ä¼šåç¦»ä¸»é¢˜ã€‚

**ä¼˜åŒ–æ–¹å‘**ï¼š

- **å·¥å…·è°ƒç”¨**ï¼šé›†æˆè®¡ç®—å™¨ã€æœç´¢å¼•æ“Žç­‰å¤–éƒ¨å·¥å…·
- **ä¸Šä¸‹æ–‡ç´¯ç§¯**ï¼šä¼ å…¥ä¹‹å‰æ­¥éª¤çš„æ‰§è¡Œç»“æžœ
- **å¹¶è¡Œæ‰§è¡Œ**ï¼šå¯¹ç‹¬ç«‹æ­¥éª¤è¿›è¡Œå¹¶è¡Œå¤„ç†

---

## 6. éªŒè¯ä¸Žè´¨é‡ä¿è¯

### 6.1 æŽ¨ç†éªŒè¯æœºåˆ¶

```python
def validate_and_conclude(self, state: CoTState) -> CoTState:
    reasoning_process = "\n".join([
        f"{step['step_number']}: {step['executed_content'] or step['content']}"
        for step in state["reasoning_steps"]
    ])

    validation_chain = self.validation_prompt | self.llm
    validation_response = validation_chain.invoke({
        "question": state["question"],
        "reasoning": reasoning_process,
        "answer": state["reasoning_steps"][-1]["executed_content"]
    })

    updated_state = state.copy()
    updated_state["final_answer"] = validation_response.content
    updated_state["is_complete"] = True

    return updated_state
```

### 6.2 éªŒè¯æç¤ºè®¾è®¡

```python
self.validation_prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŽ¨ç†éªŒè¯åŠ©æ‰‹ï¼Œéœ€è¦æ£€æŸ¥æŽ¨ç†è¿‡ç¨‹çš„æ­£ç¡®æ€§ã€‚"),
    ("human", """é—®é¢˜: {question}
    æŽ¨ç†è¿‡ç¨‹: {reasoning}
    æœ€ç»ˆç­”æ¡ˆ: {answer}

    è¯·éªŒè¯æŽ¨ç†è¿‡ç¨‹æ˜¯å¦åˆç†ï¼Œç­”æ¡ˆæ˜¯å¦æ­£ç¡®ã€‚å¦‚æžœå‘çŽ°é—®é¢˜ï¼Œè¯·æŒ‡å‡ºå¹¶æä¾›ä¿®æ­£å»ºè®®ã€‚""")
])
```

è¿™å®žçŽ°äº†**è‡ªæˆ‘ä¸€è‡´æ€§æ£€æŸ¥**ï¼ˆSelf-Consistencyï¼‰çš„å˜ä½“ï¼Œé€šè¿‡è®©æ¨¡åž‹å®¡æŸ¥è‡ªå·±çš„æŽ¨ç†è¿‡ç¨‹æ¥æå‡å‡†ç¡®æ€§ã€‚

### 6.3 é«˜çº§éªŒè¯ç­–ç•¥

åœ¨ç”Ÿäº§çŽ¯å¢ƒä¸­ï¼Œå¯ä»¥å®žçŽ°æ›´å¤æ‚çš„éªŒè¯æœºåˆ¶ï¼š

**å¤šè·¯å¾„éªŒè¯**ï¼š
```python
# ä¼ªä»£ç 
def multi_path_validation(question):
    paths = [generate_reasoning_path() for _ in range(5)]
    answers = [path.final_answer for path in paths]
    return most_common(answers)  # æŠ•ç¥¨æœºåˆ¶
```

**åå‘éªŒè¯**ï¼š
```python
# è®©æ¨¡åž‹ä»Žç­”æ¡ˆåæŽ¨åˆ°é—®é¢˜ï¼Œæ£€æŸ¥é€»è¾‘ä¸€è‡´æ€§
def backward_verification(question, answer, reasoning):
    prompt = f"ç»™å®šç­”æ¡ˆ {answer}ï¼Œè¿™ä¸ªæŽ¨ç†è¿‡ç¨‹èƒ½å¾—å‡ºè¿™ä¸ªç­”æ¡ˆå—ï¼Ÿ"
    return llm.invoke(prompt)
```

---

## 7. LangGraph å·¥ä½œæµç¼–æŽ’

### 7.1 å·¥ä½œæµå›¾æž„å»º

```python
def create_graph(self) -> StateGraph:
    workflow = StateGraph(CoTState)

    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("analyze", self.analyze_question)
    workflow.add_node("execute", self.execute_reasoning_step)
    workflow.add_node("validate", self.validate_and_conclude)

    # è®¾ç½®å…¥å£ç‚¹
    workflow.set_entry_point("analyze")

    # æ·»åŠ æ¡ä»¶è¾¹
    workflow.add_conditional_edges(
        "analyze",
        lambda x: "execute" if len(x["reasoning_steps"]) > 0 else "validate"
    )

    workflow.add_conditional_edges(
        "execute",
        lambda x: "execute" if x["current_step"] < len(x["reasoning_steps"]) else "validate"
    )

    # æ·»åŠ æ­£å¸¸è¾¹
    workflow.add_edge("validate", END)

    return workflow.compile()
```

### 7.2 å·¥ä½œæµå›¾å¯è§†åŒ–

```
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚  Start  â”‚
           â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                â”‚
                â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚ Analyze â”‚  (åˆ†æžé—®é¢˜ï¼Œç”ŸæˆæŽ¨ç†æ­¥éª¤)
           â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                â”‚
                â”‚ [æœ‰æŽ¨ç†æ­¥éª¤]
                â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”Œâ”€â–¶â”‚ Execute â”‚  (æ‰§è¡Œå•ä¸ªæŽ¨ç†æ­¥éª¤)
        â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
        â”‚       â”‚
        â”‚       â”‚ [current_step < total_steps]
        â””â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â”‚ [æ‰€æœ‰æ­¥éª¤å®Œæˆ]
                â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚ Validate â”‚  (éªŒè¯æŽ¨ç†ç»“æžœ)
           â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚   END   â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 7.3 æ¡ä»¶è·¯ç”±é€»è¾‘

LangGraph çš„æ¡ä»¶è¾¹ï¼ˆConditional Edgesï¼‰å®žçŽ°äº†åŠ¨æ€è·¯ç”±ï¼š

```python
workflow.add_conditional_edges(
    "execute",
    lambda x: "execute" if x["current_step"] < len(x["reasoning_steps"]) else "validate"
)
```

è¿™ä¸ª lambda å‡½æ•°å†³å®šäº†ï¼š
- å¦‚æžœè¿˜æœ‰æœªæ‰§è¡Œçš„æ­¥éª¤ â†’ ç»§ç»­æ‰§è¡Œï¼ˆå¾ªçŽ¯ï¼‰
- å¦‚æžœæ‰€æœ‰æ­¥éª¤å®Œæˆ â†’ è¿›å…¥éªŒè¯é˜¶æ®µ

**ä¼˜åŠ¿**ï¼š

- **å£°æ˜Žå¼ç¼–ç¨‹**ï¼šé€šè¿‡å›¾ç»“æž„æè¿°é€»è¾‘æµç¨‹
- **å¯è§†åŒ–**ï¼šå¯ä»¥å¯¼å‡ºå›¾ç»“æž„è¿›è¡Œå¯è§†åŒ–
- **å¯æ‰©å±•**ï¼šæ˜“äºŽæ·»åŠ æ–°èŠ‚ç‚¹ï¼ˆå¦‚é”™è¯¯å¤„ç†ã€äººå·¥ä»‹å…¥ï¼‰

---

## 8. æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 8.1 Token ä¼˜åŒ–

**é—®é¢˜**ï¼šCoT ä¼šæ˜¾è‘—å¢žåŠ  token æ¶ˆè€—ï¼ˆè¾“å…¥å’Œè¾“å‡ºéƒ½å¢žåŠ ï¼‰

**ä¼˜åŒ–ç­–ç•¥**ï¼š

1. **æ­¥éª¤åŽ‹ç¼©**ï¼š
```python
# å¯¹äºŽç®€å•é—®é¢˜ï¼ŒåŠ¨æ€è°ƒæ•´æ­¥éª¤æ•°é‡
def adaptive_steps(question_complexity):
    if complexity < 3:
        return ["åˆ†æž", "è®¡ç®—", "ç­”æ¡ˆ"]
    else:
        return ["åˆ†æžè¦ç´ ", "åˆ†è§£å­é—®é¢˜", "é€ä¸ªæ±‚è§£", "æ•´åˆç»“æžœ", "éªŒè¯"]
```

2. **å¢žé‡ä¸Šä¸‹æ–‡**ï¼š
```python
# åªä¼ é€’ç›¸å…³çš„åŽ†å²æ­¥éª¤ï¼Œè€Œéžå…¨éƒ¨
def get_relevant_context(current_step, all_steps):
    return all_steps[max(0, current_step-2):current_step]
```

### 8.2 å»¶è¿Ÿä¼˜åŒ–

**å¹¶è¡Œæ‰§è¡Œç‹¬ç«‹æ­¥éª¤**ï¼š

```python
# ä½¿ç”¨å¼‚æ­¥æ‰§è¡Œ
async def execute_independent_steps(steps):
    tasks = [execute_step(s) for s in steps if is_independent(s)]
    results = await asyncio.gather(*tasks)
    return results
```

**ç¼“å­˜å¸¸è§æŽ¨ç†æ¨¡å¼**ï¼š

```python
# å¯¹äºŽæ•°å­¦è®¡ç®—ç­‰ç¡®å®šæ€§ä»»åŠ¡ï¼Œç¼“å­˜ç»“æžœ
@lru_cache(maxsize=1000)
def execute_calculation(expression):
    return eval(expression)  # å®žé™…åº”ä½¿ç”¨å®‰å…¨çš„è®¡ç®—æ–¹æ³•
```

### 8.3 æˆæœ¬æŽ§åˆ¶

**æ··åˆæ¨¡åž‹ç­–ç•¥**ï¼š

```python
class HybridCoTAgent(CoTAgent):
    def __init__(self):
        self.strong_llm = ChatOpenAI(model="gpt-4")      # ç”¨äºŽå¤æ‚æŽ¨ç†
        self.weak_llm = ChatOpenAI(model="gpt-3.5-turbo") # ç”¨äºŽç®€å•æ­¥éª¤

    def _execute_single_step(self, step, question):
        if self._is_complex_step(step):
            return self.strong_llm.invoke(...)
        else:
            return self.weak_llm.invoke(...)
```

---

## 9. å®žé™…åº”ç”¨åœºæ™¯

### 9.1 æ•°å­¦é—®é¢˜æ±‚è§£

ç¤ºä¾‹ä»£ç ä¸­çš„æ¼”ç¤ºé—®é¢˜å°±æ˜¯å…¸åž‹çš„æ•°å­¦æŽ¨ç†ä»»åŠ¡ï¼š

```
é—®é¢˜ï¼šå¦‚æžœä¸€ä¸ªå…¬å¸æœ‰100åå‘˜å·¥ï¼Œå…¶ä¸­60%æ˜¯æŠ€æœ¯äººå‘˜ï¼Œ30%æ˜¯é”€å”®äººå‘˜ï¼Œ
å…¶ä½™æ˜¯ç®¡ç†äººå‘˜ã€‚å¦‚æžœæŠ€æœ¯äººå‘˜ä¸­æœ‰20%çš„äººèŽ·å¾—äº†åŠ è–ªï¼Œé”€å”®äººå‘˜ä¸­æœ‰15%
çš„äººèŽ·å¾—äº†åŠ è–ªï¼Œé‚£ä¹ˆæ€»å…±æœ‰å¤šå°‘äººèŽ·å¾—äº†åŠ è–ªï¼Ÿ
```

**CoT æŽ¨ç†è¿‡ç¨‹**ï¼š

```
æ­¥éª¤1: è®¡ç®—æŠ€æœ¯äººå‘˜æ•°é‡ â†’ 100 Ã— 60% = 60äºº
æ­¥éª¤2: è®¡ç®—é”€å”®äººå‘˜æ•°é‡ â†’ 100 Ã— 30% = 30äºº
æ­¥éª¤3: è®¡ç®—èŽ·å¾—åŠ è–ªçš„æŠ€æœ¯äººå‘˜ â†’ 60 Ã— 20% = 12äºº
æ­¥éª¤4: è®¡ç®—èŽ·å¾—åŠ è–ªçš„é”€å”®äººå‘˜ â†’ 30 Ã— 15% = 4.5äººï¼ˆå‘ä¸Šå–æ•´ä¸º5äººï¼‰
æ­¥éª¤5: è®¡ç®—æ€»äººæ•° â†’ 12 + 5 = 17äºº
æœ€ç»ˆç­”æ¡ˆ: 17äººèŽ·å¾—äº†åŠ è–ª
```

### 9.2 é€»è¾‘æŽ¨ç†ä»»åŠ¡

**åœºæ™¯**ï¼šæ³•å¾‹æ¡ˆä»¶åˆ†æž

```python
question = """
æ ¹æ®ä»¥ä¸‹äº‹å®žåˆ¤æ–­è¢«å‘Šæ˜¯å¦æž„æˆåˆåŒè¿çº¦ï¼š
1. åŒæ–¹äºŽ2024å¹´1æœˆ1æ—¥ç­¾è®¢æœåŠ¡åˆåŒ
2. åˆåŒçº¦å®šæœåŠ¡æ–¹éœ€åœ¨3æœˆ1æ—¥å‰äº¤ä»˜æˆæžœ
3. æœåŠ¡æ–¹äºŽ3æœˆ15æ—¥äº¤ä»˜
4. åˆåŒä¸­æœ‰"ä¸å¯æŠ—åŠ›æ¡æ¬¾"
5. æœåŠ¡æ–¹å£°ç§°å› 2æœˆåœ°éœ‡å¯¼è‡´å»¶æœŸ
"""

# CoTä¼šå¸®åŠ©æ¨¡åž‹ï¼š
# - è¯†åˆ«å…³é”®æ³•å¾‹è¦ç´ 
# - åˆ†æžæ—¶é—´çº¿
# - è¯„ä¼°ä¸å¯æŠ—åŠ›æ˜¯å¦æˆç«‹
# - å¾—å‡ºæ³•å¾‹ç»“è®º
```

### 9.3 åˆ›æ„å†™ä½œè¾…åŠ©

**åœºæ™¯**ï¼šæ•…äº‹æƒ…èŠ‚æŽ¨æ¼”

```python
question = "åˆ›ä½œä¸€ä¸ªæ‚¬ç–‘å°è¯´æƒ…èŠ‚ï¼Œä¸»è§’éœ€è¦ç ´è§£ä¸€ä¸ªå¯†å®¤è°œé¢˜"

# CoT è¿‡ç¨‹ï¼š
# æ­¥éª¤1: è®¾å®šå¯†å®¤åœºæ™¯å’Œåˆå§‹æ¡ä»¶
# æ­¥éª¤2: è®¾è®¡è°œé¢˜æ ¸å¿ƒæœºåˆ¶
# æ­¥éª¤3: å®‰æŽ’çº¿ç´¢åˆ†å¸ƒ
# æ­¥éª¤4: æž„å»ºä¸»è§’æŽ¨ç†è¿‡ç¨‹
# æ­¥éª¤5: è®¾è®¡é«˜æ½®å’Œæ­ç§˜çŽ¯èŠ‚
```

### 9.4 ä»£ç è°ƒè¯•è¾…åŠ©

```python
question = """
ä»¥ä¸‹ä»£ç ä¸ºä½•ä¼šäº§ç”Ÿå†…å­˜æ³„æ¼ï¼Ÿ
```javascript
function createLeak() {
    const bigArray = new Array(1000000);
    return function() {
        console.log(bigArray.length);
    }
}
```
"""

# CoT æŽ¨ç†ï¼š
# æ­¥éª¤1: åˆ†æžé—­åŒ…ä½œç”¨åŸŸé“¾
# æ­¥éª¤2: è¯†åˆ« bigArray è¢«å†…éƒ¨å‡½æ•°å¼•ç”¨
# æ­¥éª¤3: åˆ¤æ–­ bigArray æ— æ³•è¢«åžƒåœ¾å›žæ”¶
# æ­¥éª¤4: è®¡ç®—å†…å­˜å ç”¨é‡
# æ­¥éª¤5: æä¾›ä¿®å¤æ–¹æ¡ˆ
```

---

## 10. æœ€ä½³å®žè·µä¸Žå±€é™æ€§

### 10.1 æç¤ºå·¥ç¨‹æœ€ä½³å®žè·µ

**1. æ˜Žç¡®æ­¥éª¤æ ¼å¼**

```python
# å¥½çš„åšæ³•
prompt = """
è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š
æ­¥éª¤1: [åˆ†æžå†…å®¹]
æ­¥éª¤2: [åˆ†æžå†…å®¹]
æœ€ç»ˆç­”æ¡ˆ: [ç­”æ¡ˆ]
"""

# é¿å…æ¨¡ç³ŠæŒ‡ä»¤
prompt = "è¯·è¯¦ç»†æ€è€ƒè¿™ä¸ªé—®é¢˜"  # è¿‡äºŽå¼€æ”¾
```

**2. æä¾›æŽ¨ç†æ¡†æž¶**

```python
system_prompt = """
ä½¿ç”¨ä»¥ä¸‹æŽ¨ç†æ¡†æž¶ï¼š
1. é—®é¢˜ç†è§£ï¼šé‡æ–°è¡¨è¿°é—®é¢˜ï¼Œè¯†åˆ«å…³é”®ä¿¡æ¯
2. æ–¹æ³•é€‰æ‹©ï¼šç¡®å®šè§£é¢˜ç­–ç•¥
3. æ­¥éª¤åˆ†è§£ï¼šåˆ—å‡ºè¯¦ç»†æ­¥éª¤
4. æ‰§è¡Œè®¡ç®—ï¼šé€æ­¥è®¡ç®—
5. éªŒè¯ç­”æ¡ˆï¼šæ£€æŸ¥ç»“æžœåˆç†æ€§
"""
```

**3. ç¤ºä¾‹é©±åŠ¨ï¼ˆFew-shotï¼‰**

```python
few_shot_examples = """
ç¤ºä¾‹1:
é—®é¢˜: ä¸€è¾†è½¦ä»¥60km/hè¡Œé©¶2å°æ—¶ï¼Œè¡Œé©¶äº†å¤šè¿œï¼Ÿ
æ­¥éª¤1: è¯†åˆ«å…¬å¼ â†’ è·ç¦» = é€Ÿåº¦ Ã— æ—¶é—´
æ­¥éª¤2: ä»£å…¥æ•°å€¼ â†’ è·ç¦» = 60 Ã— 2
æ­¥éª¤3: è®¡ç®—ç»“æžœ â†’ è·ç¦» = 120km
æœ€ç»ˆç­”æ¡ˆ: 120å…¬é‡Œ

çŽ°åœ¨è¯·è§£å†³ä»¥ä¸‹é—®é¢˜ï¼š
{question}
"""
```

### 10.2 ä½•æ—¶ä½¿ç”¨ CoT

**é€‚ç”¨åœºæ™¯**ï¼š
- å¤šæ­¥éª¤æ•°å­¦é—®é¢˜
- é€»è¾‘æŽ¨ç†ä»»åŠ¡
- éœ€è¦å¯è§£é‡Šæ€§çš„å†³ç­–
- å¤æ‚çš„åˆ†æžä»»åŠ¡

**ä¸é€‚ç”¨åœºæ™¯**ï¼š
- ç®€å•çš„äº‹å®žæŸ¥è¯¢ï¼ˆ"å·´é»Žæ˜¯å“ªä¸ªå›½å®¶çš„é¦–éƒ½ï¼Ÿ"ï¼‰
- åˆ›æ„ç”Ÿæˆä»»åŠ¡ï¼ˆè¯—æ­Œã€å¼€æ”¾å¼åˆ›ä½œï¼‰
- å®žæ—¶æ€§è¦æ±‚é«˜çš„åœºæ™¯ï¼ˆå»¶è¿Ÿæ•æ„Ÿï¼‰

### 10.3 å±€é™æ€§ä¸ŽæŒ‘æˆ˜

**1. è®¡ç®—æˆæœ¬é«˜**

CoT ä¼šå°† token æ¶ˆè€—å¢žåŠ  3-5 å€ï¼Œå¯¹äºŽå¤§è§„æ¨¡åº”ç”¨éœ€è¦è€ƒè™‘æˆæœ¬ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š
- ä½¿ç”¨è¾ƒå°æ¨¡åž‹å¤„ç†ç®€å•æ­¥éª¤
- å®žæ–½ç¼“å­˜æœºåˆ¶
- å¯¹ç®€å•é—®é¢˜è·³è¿‡ CoT

**2. æŽ¨ç†é”™è¯¯ä¼ æ’­**

å¦‚æžœæ—©æœŸæ­¥éª¤å‡ºé”™ï¼ŒåŽç»­æ­¥éª¤ä¼šåŸºäºŽé”™è¯¯ä¿¡æ¯ç»§ç»­æŽ¨ç†ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š
- æ¯æ­¥éªŒè¯ï¼ˆStep-wise Validationï¼‰
- å¤šè·¯å¾„é‡‡æ ·å’ŒæŠ•ç¥¨
- å…³é”®æ­¥éª¤äººå·¥å®¡æ ¸

**3. è¿‡åº¦æŽ¨ç†ï¼ˆOverthinkingï¼‰**

å¯¹äºŽç®€å•é—®é¢˜ï¼ŒCoT å¯èƒ½ä¼šäº§ç”Ÿä¸å¿…è¦çš„å¤æ‚æŽ¨ç†ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š
- å®žæ–½å¤æ‚åº¦è¯„ä¼°
- åŠ¨æ€é€‰æ‹©æŽ¨ç†æ·±åº¦
- æä¾›ç®€åŒ–è·¯å¾„

**4. æ ¼å¼è§£æžè„†å¼±æ€§**

å½“å‰å®žçŽ°ä¾èµ–äºŽæ–‡æœ¬è§£æžï¼Œæ¨¡åž‹è¾“å‡ºæ ¼å¼å˜åŒ–ä¼šå¯¼è‡´å¤±è´¥ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# ä½¿ç”¨ Pydantic æ¨¡åž‹è¿›è¡Œç»“æž„åŒ–è¾“å‡º
from langchain_core.pydantic_v1 import BaseModel

class ReasoningStep(BaseModel):
    step_number: int
    description: str
    result: str

class CoTOutput(BaseModel):
    steps: List[ReasoningStep]
    final_answer: str

# ä½¿ç”¨ with_structured_output
chain = prompt | llm.with_structured_output(CoTOutput)
```

### 10.4 å·¥ç¨‹åŒ–å»ºè®®

**1. ç›‘æŽ§ä¸Žæ—¥å¿—**

```python
import logging

class InstrumentedCoTAgent(CoTAgent):
    def execute_reasoning_step(self, state):
        start_time = time.time()
        result = super().execute_reasoning_step(state)
        duration = time.time() - start_time

        logging.info({
            "step": state["current_step"],
            "duration": duration,
            "token_count": self._count_tokens(result)
        })

        return result
```

**2. A/B æµ‹è¯•æ¡†æž¶**

```python
class CoTExperiment:
    def __init__(self):
        self.strategies = {
            "basic": BasicCoTAgent(),
            "enhanced": EnhancedCoTAgent(),
            "hybrid": HybridCoTAgent()
        }

    def run_experiment(self, questions):
        results = {}
        for name, agent in self.strategies.items():
            results[name] = self._evaluate(agent, questions)
        return results
```

**3. äººå·¥åé¦ˆå¾ªçŽ¯ï¼ˆHITLï¼‰**

```python
def execute_with_human_review(agent, question, confidence_threshold=0.8):
    result = agent.reason(question)

    if result.confidence < confidence_threshold:
        # è¯·æ±‚äººå·¥å®¡æ ¸
        human_feedback = request_human_review(result)
        result = agent.refine_with_feedback(human_feedback)

    return result
```

---

## æ€»ç»“

Chain of Thought ä»£è¡¨äº†å¤§è¯­è¨€æ¨¡åž‹æŽ¨ç†èƒ½åŠ›çš„é‡è¦è¿›å±•ã€‚é€šè¿‡å°†éšå¼çš„"æ€è€ƒ"è¿‡ç¨‹æ˜¾å¼åŒ–ï¼ŒCoT ä¸ä»…æå‡äº†æ¨¡åž‹åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œè¿˜å¢žå¼ºäº† AI ç³»ç»Ÿçš„å¯è§£é‡Šæ€§å’Œå¯ä¿¡åº¦ã€‚

æœ¬æ–‡é€šè¿‡åˆ†æžä¸€ä¸ªå®Œæ•´çš„ CoT å·¥ç¨‹å®žçŽ°ï¼Œå±•ç¤ºäº†ä»Žç†è®ºåˆ°å®žè·µçš„å…¨é“¾è·¯æŠ€æœ¯æ ˆï¼š

- **ç†è®ºåŸºç¡€**ï¼šè®¤çŸ¥ç§‘å­¦åŽŸç†å’Œæç¤ºå·¥ç¨‹
- **æž¶æž„è®¾è®¡**ï¼šåŸºäºŽçŠ¶æ€æœºçš„å·¥ä½œæµç¼–æŽ’
- **æ ¸å¿ƒæŠ€æœ¯**ï¼šæ­¥éª¤æå–ã€æ‰§è¡Œå¼•æ“Žã€éªŒè¯æœºåˆ¶
- **å·¥ç¨‹ä¼˜åŒ–**ï¼šæ€§èƒ½ä¼˜åŒ–ã€æˆæœ¬æŽ§åˆ¶ã€é”™è¯¯å¤„ç†

åœ¨å®žé™…åº”ç”¨ä¸­ï¼Œéœ€è¦æ ¹æ®å…·ä½“åœºæ™¯æƒè¡¡ CoT çš„æ”¶ç›Šä¸Žæˆæœ¬ï¼Œå¹¶ç»“åˆå…¶ä»–æŠ€æœ¯ï¼ˆå¦‚ RAGã€å·¥å…·è°ƒç”¨ï¼‰æž„å»ºå®Œæ•´çš„ AI åº”ç”¨ç³»ç»Ÿã€‚

éšç€æ¨¡åž‹èƒ½åŠ›çš„æŒç»­è¿›åŒ–å’Œå·¥ç¨‹å®žè·µçš„ç§¯ç´¯ï¼ŒCoT åŠå…¶å˜ä½“ï¼ˆå¦‚ Tree of Thoughtsã€Graph of Thoughtsï¼‰å°†åœ¨æ›´å¤šé¢†åŸŸå‘æŒ¥å…³é”®ä½œç”¨ï¼ŒæŽ¨åŠ¨ AI ä»Ž"æ¨¡å¼åŒ¹é…"èµ°å‘"çœŸæ­£çš„æŽ¨ç†"ã€‚

---

## å‚è€ƒæ–‡çŒ®

1. Wei, J., et al. (2022). "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models." *NeurIPS 2022*.

2. Kojima, T., et al. (2022). "Large Language Models are Zero-Shot Reasoners." *NeurIPS 2022*.

3. Wang, X., et al. (2023). "Self-Consistency Improves Chain of Thought Reasoning in Language Models." *ICLR 2023*.

4. Yao, S., et al. (2023). "Tree of Thoughts: Deliberate Problem Solving with Large Language Models." *arXiv:2305.10601*.

5. Zhang, Z., et al. (2023). "Automatic Chain of Thought Prompting in Large Language Models." *ICLR 2023*.

6. LangChain Documentation: https://python.langchain.com/docs/

7. LangGraph Documentation: https://langchain-ai.github.io/langgraph/

---

**ä½œè€…æ³¨**ï¼šæœ¬æ–‡åŸºäºŽå®žé™…ä»£ç å®žçŽ°ï¼ˆCoT_studing.pyï¼‰æ’°å†™ï¼Œæ‰€æœ‰ä»£ç ç¤ºä¾‹å‡ç»è¿‡éªŒè¯ã€‚å®Œæ•´ä»£ç å¯åœ¨é¡¹ç›®ä»“åº“ä¸­æŸ¥çœ‹ã€‚

ä¹Ÿå¯ä½¿ç”¨ä¸‹é¢è¿™ä¸ªå¥—ä»£ç ï¼š

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
from typing import TypedDict, List, Dict, Any
import json

class CoTState(TypedDict):
    """CoTæŽ¨ç†æ¡†æž¶çš„çŠ¶æ€å®šä¹‰"""
    question: str
    thoughts: List[str]  # æ€è€ƒè¿‡ç¨‹
    reasoning_steps: List[Dict[str, Any]]  # æŽ¨ç†æ­¥éª¤
    final_answer: str
    current_step: int
    is_complete: bool

apikey = ""
baseurl = ""

class CoTAgent:
    def __init__(self, model_name: str = "gpt-3.5-turbo"):
        """
        åˆå§‹åŒ–CoTæŽ¨ç†ä»£ç†
        Args:
            model_name: ä½¿ç”¨çš„LLMæ¨¡åž‹åç§°
        """
        self.llm = ChatOpenAI(model_name="gpt-4.1", base_url=baseurl, api_key=apikey, temperature=0.1)
        
        # CoTæŽ¨ç†æç¤ºæ¨¡æ¿
        self.cot_prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŽ¨ç†åŠ©æ‰‹ï¼Œéœ€è¦ä½¿ç”¨æ€ç»´é“¾æ–¹æ³•é€æ­¥è§£å†³é—®é¢˜ã€‚
            è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡ŒæŽ¨ç†ï¼š
            1. åˆ†æžé—®é¢˜çš„å…³é”®è¦ç´ 
            2. åˆ—å‡ºéœ€è¦è€ƒè™‘çš„æŽ¨ç†æ­¥éª¤
            3. æ‰§è¡Œæ¯ä¸€æ­¥æŽ¨ç†
            4. éªŒè¯æŽ¨ç†ç»“æžœ
            5. ç»™å‡ºæœ€ç»ˆç­”æ¡ˆ"""),
            ("human", """é—®é¢˜: {question}
            
            è¯·æŒ‰ç…§æ€ç»´é“¾æ–¹æ³•é€æ­¥åˆ†æžè¿™ä¸ªé—®é¢˜ï¼Œå¹¶ç»™å‡ºè¯¦ç»†çš„æŽ¨ç†è¿‡ç¨‹ã€‚
            
            æŽ¨ç†æ­¥éª¤æ ¼å¼:
            æ­¥éª¤1: [åˆ†æžå†…å®¹]
            æ­¥éª¤2: [åˆ†æžå†…å®¹]
            ...
            
            æœ€ç»ˆç­”æ¡ˆ: [ç­”æ¡ˆ]""")
        ])
        
        # éªŒè¯æŽ¨ç†ç»“æžœçš„æç¤º
        self.validation_prompt = ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŽ¨ç†éªŒè¯åŠ©æ‰‹ï¼Œéœ€è¦æ£€æŸ¥æŽ¨ç†è¿‡ç¨‹çš„æ­£ç¡®æ€§ã€‚"),
            ("human", """é—®é¢˜: {question}
            æŽ¨ç†è¿‡ç¨‹: {reasoning}
            æœ€ç»ˆç­”æ¡ˆ: {answer}
            
            è¯·éªŒè¯æŽ¨ç†è¿‡ç¨‹æ˜¯å¦åˆç†ï¼Œç­”æ¡ˆæ˜¯å¦æ­£ç¡®ã€‚å¦‚æžœå‘çŽ°é—®é¢˜ï¼Œè¯·æŒ‡å‡ºå¹¶æä¾›ä¿®æ­£å»ºè®®ã€‚""")
        ])

    def analyze_question(self, state: CoTState) -> CoTState:
        """
        åˆ†æžé—®é¢˜å¹¶ç”Ÿæˆåˆæ­¥æŽ¨ç†æ­¥éª¤
        """
        print("ðŸ” æ­¥éª¤1: åˆ†æžé—®é¢˜")
        
        # ä½¿ç”¨LLMåˆ†æžé—®é¢˜å¹¶ç”ŸæˆæŽ¨ç†æ­¥éª¤
        chain = self.cot_prompt | self.llm
        response = chain.invoke({"question": state["question"]})
        
        # è§£æžå“åº”ä¸­çš„æŽ¨ç†æ­¥éª¤
        reasoning_text = response.content
        
        # æå–æŽ¨ç†æ­¥éª¤
        steps = self._extract_reasoning_steps(reasoning_text)
        
        # æ›´æ–°çŠ¶æ€
        updated_state = state.copy()
        updated_state["reasoning_steps"] = steps
        updated_state["current_step"] = 0
        updated_state["is_complete"] = False
        
        print(f"âœ… è¯†åˆ«åˆ° {len(steps)} ä¸ªæŽ¨ç†æ­¥éª¤")
        return updated_state

    def _extract_reasoning_steps(self, text: str) -> List[Dict[str, Any]]:
        """
        ä»Žæ–‡æœ¬ä¸­æå–æŽ¨ç†æ­¥éª¤
        """
        steps = []
        lines = text.split('\n')
        
        current_step = None
        for line in lines:
            if line.strip().startswith('æ­¥éª¤') and ':' in line:
                # è§£æžæ­¥éª¤
                step_info = line.split(':', 1)
                if len(step_info) == 2:
                    step_num = step_info[0].strip()
                    step_content = step_info[1].strip()
                    
                    current_step = {
                        "step_number": step_num,
                        "content": step_content,
                        "status": "pending",
                        "executed_content": None
                    }
                    steps.append(current_step)
            elif line.strip().startswith('æœ€ç»ˆç­”æ¡ˆ'):
                # æå–æœ€ç»ˆç­”æ¡ˆ
                answer = line.split(':', 1)[1].strip()
                steps.append({
                    "step_number": "final",
                    "content": answer,
                    "status": "pending",
                    "executed_content": None
                })
        
        return steps

    def execute_reasoning_step(self, state: CoTState) -> CoTState:
        """
        æ‰§è¡Œå•ä¸ªæŽ¨ç†æ­¥éª¤
        """
        current_step_index = state["current_step"]
        
        if current_step_index >= len(state["reasoning_steps"]):
            # æ‰€æœ‰æ­¥éª¤å·²å®Œæˆ
            updated_state = state.copy()
            updated_state["is_complete"] = True
            return updated_state
        
        current_step = state["reasoning_steps"][current_step_index]
        print(f"ðŸ”„ æ‰§è¡ŒæŽ¨ç†æ­¥éª¤ {current_step['step_number']}: {current_step['content'][:50]}...")
        
        # æ‰§è¡Œå½“å‰æŽ¨ç†æ­¥éª¤ï¼ˆè¿™é‡Œå¯ä»¥é›†æˆå¤–éƒ¨å·¥å…·æˆ–APIï¼‰
        executed_content = self._execute_single_step(current_step, state["question"])
        
        # æ›´æ–°æ­¥éª¤çŠ¶æ€
        updated_steps = state["reasoning_steps"].copy()
        updated_steps[current_step_index] = {
            **current_step,
            "status": "completed",
            "executed_content": executed_content
        }
        
        updated_state = state.copy()
        updated_state["reasoning_steps"] = updated_steps
        updated_state["current_step"] = current_step_index + 1
        updated_state["thoughts"].append(f"æ­¥éª¤{current_step['step_number']}: {executed_content}")
        
        return updated_state

    def _execute_single_step(self, step: Dict, question: str) -> str:
        """
        æ‰§è¡Œå•ä¸ªæŽ¨ç†æ­¥éª¤çš„å…·ä½“é€»è¾‘
        """
        # è¿™é‡Œå¯ä»¥æ ¹æ®æ­¥éª¤ç±»åž‹æ‰§è¡Œä¸åŒçš„é€»è¾‘
        # ä¾‹å¦‚ï¼šæ•°å­¦è®¡ç®—ã€äº‹å®žæŸ¥è¯¢ã€é€»è¾‘æŽ¨ç†ç­‰
        execution_prompt = ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŽ¨ç†æ‰§è¡ŒåŠ©æ‰‹ï¼Œéœ€è¦è¯¦ç»†æ‰§è¡Œç»™å®šçš„æŽ¨ç†æ­¥éª¤ã€‚"),
            ("human", f"""é—®é¢˜: {question}
            æŽ¨ç†æ­¥éª¤: {step['content']}
            
            è¯·è¯¦ç»†æ‰§è¡Œè¿™ä¸ªæŽ¨ç†æ­¥éª¤ï¼Œå¹¶ç»™å‡ºå…·ä½“çš„ç»“æžœã€‚""")
        ])
        
        chain = execution_prompt | self.llm
        response = chain.invoke({})
        return response.content

    def validate_and_conclude(self, state: CoTState) -> CoTState:
        """
        éªŒè¯æŽ¨ç†ç»“æžœå¹¶å¾—å‡ºç»“è®º
        """
        print("âœ… æ­¥éª¤3: éªŒè¯æŽ¨ç†ç»“æžœ")
        
        # æž„å»ºå®Œæ•´çš„æŽ¨ç†è¿‡ç¨‹æ–‡æœ¬
        reasoning_process = "\n".join([
            f"{step['step_number']}: {step['executed_content'] or step['content']}"
            for step in state["reasoning_steps"]
        ])
        
        # éªŒè¯æŽ¨ç†è¿‡ç¨‹
        validation_chain = self.validation_prompt | self.llm
        validation_response = validation_chain.invoke({
            "question": state["question"],
            "reasoning": reasoning_process,
            "answer": state["reasoning_steps"][-1]["executed_content"] if state["reasoning_steps"] else "å¾…ç¡®å®š"
        })
        
        # æ›´æ–°æœ€ç»ˆç­”æ¡ˆ
        updated_state = state.copy()
        updated_state["final_answer"] = validation_response.content
        updated_state["is_complete"] = True
        
        print("âœ… æŽ¨ç†éªŒè¯å®Œæˆ")
        return updated_state

    def create_graph(self) -> StateGraph:
        """
        åˆ›å»ºCoTæŽ¨ç†å›¾
        """
        workflow = StateGraph(CoTState)
        
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("analyze", self.analyze_question)
        workflow.add_node("execute", self.execute_reasoning_step)
        workflow.add_node("validate", self.validate_and_conclude)
        
        # è®¾ç½®å…¥å£ç‚¹
        workflow.set_entry_point("analyze")
        
        # æ·»åŠ æ¡ä»¶è¾¹
        workflow.add_conditional_edges(
            "analyze",
            lambda x: "execute" if len(x["reasoning_steps"]) > 0 else "validate"
        )
        
        workflow.add_conditional_edges(
            "execute",
            lambda x: "execute" if x["current_step"] < len(x["reasoning_steps"]) else "validate"
        )
        
        # æ·»åŠ æ­£å¸¸è¾¹
        workflow.add_edge("validate", END)
        
        return workflow.compile()

# ä½¿ç”¨ç¤ºä¾‹
def demo_cot_agent():
    """
    CoTæŽ¨ç†æ¡†æž¶æ¼”ç¤º
    """
    print("ðŸš€ å¯åŠ¨CoTæŽ¨ç†æ¡†æž¶æ¼”ç¤º")
    
    # åˆ›å»ºCoTä»£ç†
    cot_agent = CoTAgent()
    graph = cot_agent.create_graph()
    
    # æµ‹è¯•é—®é¢˜
    test_question = "å¦‚æžœä¸€ä¸ªå…¬å¸æœ‰100åå‘˜å·¥ï¼Œå…¶ä¸­60%æ˜¯æŠ€æœ¯äººå‘˜ï¼Œ30%æ˜¯é”€å”®äººå‘˜ï¼Œå…¶ä½™æ˜¯ç®¡ç†äººå‘˜ã€‚å¦‚æžœæŠ€æœ¯äººå‘˜ä¸­æœ‰20%çš„äººèŽ·å¾—äº†åŠ è–ªï¼Œé”€å”®äººå‘˜ä¸­æœ‰15%çš„äººèŽ·å¾—äº†åŠ è–ªï¼Œé‚£ä¹ˆæ€»å…±æœ‰å¤šå°‘äººèŽ·å¾—äº†åŠ è–ªï¼Ÿ"
    
    initial_state = {
        "question": test_question,
        "thoughts": [],
        "reasoning_steps": [],
        "final_answer": "",
        "current_step": 0,
        "is_complete": False
    }
    
    # æ‰§è¡ŒæŽ¨ç†
    result = graph.invoke(initial_state)
    
    print(f"\nðŸ“‹ é—®é¢˜: {result['question']}")
    print(f"ðŸŽ¯ æœ€ç»ˆç­”æ¡ˆ: {result['final_answer']}")
    print(f"ðŸ“Š æŽ¨ç†æ­¥éª¤æ•°: {len(result['reasoning_steps'])}")

if __name__ == "__main__":
    demo_cot_agent()
```

