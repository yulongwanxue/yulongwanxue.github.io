

# 选择模型基座->LLama-Factory微调（LORA）->vLLM部署

流程：

你的方案是“对的路线”，而且在工业里很常见：**HF 选底模 → LLaMA-Factory 做（Q）LoRA 微调 → vLLM 以 OpenAI 兼容 API 部署**。下面给你一份从选型到上线的**落地清单 + 常见坑位**，以及可直接用的命令模板。

------

## 一、选底模（HF）

**要点**

- **License**：确认可商用（如 Llama-3.1/3.2 家族要看 Meta 许可，Qwen/GLM 有各自条款）。
- **上下文长度**：预期要长上下文就优先选原生支持 ≥128k 的版本（或至少 8k/32k 且支持 RoPE scaling）。
- **聊天模板**：确认 `tokenizer_config.json` 的 `chat_template`。后续推理与训练**必须一致**，否则对话行为会跑偏。
- **多语言/领域**：尽量选**近两年**发布、指令对齐较好的基座，省下大量对齐成本。

------

## 二、LLaMA-Factory 微调（LoRA / QLoRA）

> 结论先说：**QLoRA 是训练侧省显存**的方法，**上线推理时并不等于 4bit 基座**；上线通常仍使用 **BF16/FP16 基座 + LoRA 适配器**（热插拔），或将 LoRA **合并**成全量权重。

**推荐配置（SFT）**

- 精度与显存：QLoRA（nf4/double quant）能把 7B/8B 放到更小卡上训练；全量 LoRA 则更稳但吃显存。
- LoRA 超参：`r=8~16, alpha=16~32, dropout=0.05~0.1`；target modules 选 `q_proj,k_proj,v_proj,o_proj,gate_up_proj,down_proj`（视模型实现）。
- 上下文：与未来部署 `max_model_len` 对齐。若要扩长度，统一使用 **相同 RoPE scaling 策略**（LLaMA-Factory 支持 YARN、NTK 等）。
- 数据：统一 `system/user/assistant` 模板；清洗重复与格式错误，保证 `eos` 一致性。
- 对齐：SFT 后可加 **DPO/ORPO** 少量步数提升助手行为；注意采样温度和拒答指令。

**导出**

- **PEFT/LoRA** 形态：保存为 `adapter_model.safetensors` + `adapter_config.json`（HF/PEFT 标准）。
- **权重合并（可选）**：LLaMA-Factory 支持将 LoRA 合并成全量权重（得到新的 `pytorch_model-*.safetensors`）。
  - 优点：推理简单、延迟更低；
  - 缺点：每个业务一套全量权重，存储/升级成本高；不利于多租户。

> **上线建议**：如果你计划“一底模多场景”，**优先保留 LoRA 适配器**，用 vLLM **热加载**；需要极致延迟再对稳定的那几条业务线做合并版。

------

## 三、vLLM 部署（OpenAI 兼容）

**版本要点**

- 用**较新版本**的 vLLM（≥0.5.x），支持 **PagedAttention、连续批处理、Speculative/Lookahead 解码、KV Cache 量化**，并已支持 **LoRA 热插拔**。
- **权重量化**：vLLM 原生支持 **AWQ/GPTQ** 等 weight-only 量化（不同版本支持度不同）。**QLoRA 训练得到的 4bit 不是直接可部署的基座**；如要上 4bit 推理，需另行做 AWQ/GPTQ/FP8 权重或使用支持 GGUF 的引擎（非 vLLM 常规路径）。

**最小可用命令（单 LoRA 热加载）**

```bash
python -m vllm.entrypoints.openai.api_server \
  --model /models/BaseModel  \
  --dtype bfloat16 \
  --tensor-parallel-size 1 \
  --max-model-len 8192 \
  --enforce-eager \
  --api-key your_key \
  --chat-template /models/BaseModel/tokenizer_config.json \
  --lora-modules loraA=/models/loraA \
  --port 8000
```

**多 LoRA/热插拔**

- 启动时可注册多个：`--lora-modules loraA=...,loraB=...`
- 运行时通过请求头或参数选择 LoRA（不同版本方式略有差异，常见是 `X-LoRA-Adapter: loraA` 或在 body 里加 `lora_name`/`adapter_name`）。

**OpenAI 客户端示例**

```python
from openai import OpenAI
client = OpenAI(base_url="http://localhost:8000/v1", api_key="your_key")

resp = client.chat.completions.create(
  model="BaseModel",
  messages=[
    {"role":"system","content":"You are a helpful assistant."},
    {"role":"user","content":"你好！"}
  ],
  # 选择特定 LoRA（如你的 vLLM 版本支持 body 参数）
  extra_body={"lora_name":"loraA"},
  temperature=0.7,
  stream=True
)
for chunk in resp:
    ...
```

**性能调优关键**

- `--max-model-len`: 与训练时一致；越大占用越高。
- `--gpu-memory-utilization`: 默认 0.9；根据 OOM 调整。
- `--max-num-seqs`, `--max-num-batched-tokens`: 控制并发批大小与吞吐。
- **KV Cache 量化**（如 INT8/FP8）：大幅降显存，延迟影响小。
- **Speculative/Lookahead**：配小型草稿模型或启用内部策略，提升 tokens/s。
- **张量并行**：多卡时 `--tensor-parallel-size=N`；尽量用 NVLink/PCIE4+。

------

## 四、端到端一致性 & 常见坑

1. **聊天模板/分隔符不一致**

- 训练使用了某种 `chat_template`，推理却用默认或别的模板 → 行为崩坏、拒答/越狱异常。
- **对策**：训练数据构造与推理 `--chat-template`、`apply_chat_template` 严格一致；必要时在推理侧自行模板化消息。

1. **RoPE/位置编码不匹配**

- 训练时做了 RoPE scaling（YARN/NTK），上线没带同样配置 → 长上下文退化/复读。
- **对策**：在导出或推理参数中携带相同的 scaling 策略；同一模型族不同策略不应混用。

1. **QLoRA 误区**

- 以为“QLoRA 训练 = 4bit 推理”。实际上 QLoRA 只是在**训练**期间做 4bit 权重存储/计算近似；**上线基座仍建议 BF16/FP16**（或做 AWQ/GPTQ 量化的独立推理权重）。
- **对策**：产出 LoRA 适配器用于注入；如需 weight-only 量化，另行离线量化。

1. **Tokenizer/特殊 token 改动**

- 训练时改过 `eos/bos/pad` 或添加了新 token，但推理侧 tokenizer 没同步 → 概率错位。
- **对策**：把训练用的 `tokenizer.json` 一并随模型部署；保持版本固定。

1. **合并权重的数值漂移**

- LoRA 合并后在极端温度或长上下文下表现与注入式略有差异。
- **对策**：上线前做 A/B；稳定业务用合并版，其余保留 LoRA 注入。

1. **对齐缺口**

- 只做 SFT 未对齐拒答/安全行为，线上内容风险高。
- **对策**：少量 DPO/ORPO + 规则层（Nemo Guardrails/自研拦截）+ 采样温度限制。

1. **评测与回滚**

- 仅离线指标（loss/acc）→ 实际对话质量不稳。
- **对策**：建立**离线集 + 在线 A/B**，保存上一个稳定镜像，回滚一键。

------

## 五、部署与运维建议

- **容器化**：用 CUDA 对齐的基础镜像（含 `flash-attn`/`xformers` 版本），构建 `ENTRYPOINT` 即 vLLM。
- **监控**：Prometheus/Grafana 抓 `tokens/s、TP50/95、OOM 次数、拒绝率、GPU 利用率`。
- **多租户**：路由按 `tenant_id→LoRA`，并记录审计日志（prompt/output 采样，PII 掩码）。
- **弹性**：K8s HPA 根据 `GPU Util` 与 `inflight requests` 扩缩；冷启动前置少量实例。
- **成本**：优先上 **KV Cache 量化 + 连续批处理**；能接受微小质量损失再考虑 AWQ/GPTQ。

------

## 六、你可以直接套用的参数模板

**LLaMA-Factory（示例 QLoRA SFT）**

```bash
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 src/train_bash.py \
  --stage sft --model_name_or_path /models/BaseModel \
  --template chatml --finetuning_type lora --lora_target q_proj,v_proj,k_proj,o_proj,gate_up_proj,down_proj \
  --resize_vocab False \
  --dataset my_sft.jsonl --cutoff_len 8192 \
  --per_device_train_batch_size 2 --gradient_accumulation_steps 8 \
  --learning_rate 2e-4 --num_train_epochs 3 --lr_scheduler_type cosine \
  --fp16 False --bf16 True \
  --quantization_bit 4 --bnb_4bit_quant_type nf4 --bnb_4bit_compute_dtype bfloat16 \
  --lora_r 16 --lora_alpha 32 --lora_dropout 0.05 \
  --save_steps 1000 --logging_steps 20 --evaluation_strategy "no" \
  --output_dir /outputs/loraA
```

**导出 LoRA/合并（节选）**

```bash
# 仅导出 LoRA 适配器（PEFT 形态）
python src/export_model.py --finetuning_type lora \
  --model_name_or_path /models/BaseModel --adapter_name_or_path /outputs/loraA \
  --export_dir /deploy/loraA --export_legacy_format False

# 合并为全量权重（如需）
python src/export_model.py --finetuning_type lora --merge_lora True \
  --model_name_or_path /models/BaseModel --adapter_name_or_path /outputs/loraA \
  --export_dir /deploy/mergedA
```

**vLLM（带多 LoRA、KV 量化）**

```bash
python -m vllm.entrypoints.openai.api_server \
  --model /models/BaseModel \
  --dtype bfloat16 \
  --max-model-len 8192 \
  --tensor-parallel-size 2 \
  --kv-cache-dtype fp8 \
  --gpu-memory-utilization 0.92 \
  --max-num-seqs 256 \
  --api-key your_key \
  --lora-modules loraA=/deploy/loraA,loraB=/deploy/loraB \
  --port 8000
```

------

## 七、什么时候考虑替代/补充

- **极致延迟/吞吐**：尝试 **TensorRT-LLM Runtime**（需单独导出引擎，工程复杂，但延迟和 TPS 更强）。
- **小型边缘**：Ollama/llama.cpp + GGUF；与你现有链路分支共存。

------

如果你把**目标 GPU、期望 QPS、上下文长度、是否多租户（多 LoRA）\**告诉我，我可以直接给出\**精准的 vLLM 启动参数和资源规格**（显存预估、并发/批参数、HPA 指标门限），再配一套**A/B 与回滚脚本**模板，基本即可一键上线。







## LLama-Factory部署使用：

1、